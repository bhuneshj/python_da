{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5bf6b65-ce55-4b01-9ba1-575745e8f692",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/datasets'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3328005-8979-405e-9315-98789b6c3e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(os.getcwd()+'/datasets/taxi_trip_data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12cf458-55bb-4eba-b94f-086907f43564",
   "metadata": {},
   "source": [
    "## Goal\n",
    "\n",
    "The goal of this notebook is to clean and analize the data available for the purpose of later utilizing it in machine learning algorithms to generate predictions of fare amounts for potential rides and riders.\n",
    "\n",
    "ML Goal: \n",
    "\n",
    " -  plug in specific variables and generate an output that is as close as possible to what the actual fare turns out to be.\n",
    "\n",
    "### Planning for Features\n",
    "Here are the features of the current dataset that we'll keep, as well as a few that will need to be created based on other features, this is called feature engineering:\n",
    "\n",
    "- pickup_timestamp\n",
    "- dropoff_timestamp\n",
    "- trip_distance\n",
    "- fare_amount\n",
    "- extra\n",
    "- mta_tax\n",
    "- imp_surcharge\n",
    "- total_amount\n",
    "- pickup_location_id\n",
    "- dropoff_location_id\n",
    "- \n",
    "### Exploration\n",
    "\n",
    "One of the most powerful tools I have found for quickly calculating and visualizing the correlation between different values is a correlation matrix or heatmap.\n",
    "The correlation matrix calculates how the change in one value effects a change in the other value, and assigns a value between -1 and 1 to that correlation.\n",
    "Let's review what those correlation values mean before we move on:\n",
    "\n",
    "-1 A very strong negative correlation, when value A moves in one direction, value B moves in the opposite direction.\n",
    "0 No correlation between values A and B, when one moves, the other is not effected.\n",
    "1 A very strong positive correlation, as you can guess, this is the opposite of the negative correlation above. When value A moves in one direction, value B follows in the same direction.\n",
    "\n",
    "It's important to remember that this value isn't related to the rate of change, only the direction of change. Value A moves up, and value B either stays, moves up, or moves down.\n",
    "\n",
    "Let's generate that matrix and plot it out in a heatmap!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c040c07f-a163-4d87-8f8a-901adf73ea2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating the correlation matrix\n",
    "df_corr = df.select_dtypes(include=['number']).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76a4a9c-30e0-4a28-ad62-07a084386458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drawing the heatmap\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "ax = sns.heatmap(df_corr, cmap='YlGnBu', annot=True, linewidths=0.5);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1eb6b8-d868-4ba6-9e46-ba0e3cc0515c",
   "metadata": {},
   "source": [
    "So let's first list the values we should be focusing on here, we already have a list of known values that we should keeep, the only remaining values are:\n",
    "\n",
    "vendor_id - Vendor of data provider. This definitely won't be used for anything for our model here\n",
    "\n",
    "rate_code - The rate code at the end of the trip. Used likely to track certain charges. Has a correlation with tolls and tips but not much with anything else.\n",
    "\n",
    "sotre_and_fwd_flag - This is simply a fag that indicates whether a value was stored in vehicle memory before being recorded due to a lack of internet connection. This is useless to us, however, it's currently stored as a string and converting it to a value that can appear in a correlation matrix later might serve useful. While not likely, it could be possible that values are different from those not stored in memory, such as having a higher amount of errors, or some upload process might be altering values in an unexpected way. I'll keep this for now and check for any correlations later.\n",
    "\n",
    "Okay, so in the end, we're only dropping one column right off the start and that's vendor ID. We have a lot of data to work with so any small amount of change will make a larger than usual impact, so I'll drop that now and then talk about the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb9b1db-133f-4060-a29a-fc68d3d767c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('vendor_id', axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c70d8de-006b-43b0-b2f9-cdda3c649126",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90838e67-b761-41f2-8332-6eaec683f03a",
   "metadata": {},
   "source": [
    " - Remove duplicate rows - Carefully, as we only want to remove duplicate trips, not duplicates within the values themselves. These values are not required to be unique.\n",
    "- Check for missing values\n",
    "- Check for zeros and empty strings. These values won't be \"missing\" but still aren't valid. Very few columns in this data have valid zeros\n",
    "- Validate formatting of data, especially dates\n",
    "- Strip and normalize strings - our data doesn't contain any strings, so we can skip this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fff3d4f-9db2-4baa-b343-689b76fb05d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicates - Easy first step for reducing the size of the data, making following steps quicker  \n",
    "# From here on, I'm going to rename the dataframe from df to td for temporary data, thus not altering the original dataframe until much later. \n",
    "td = df.drop_duplicates()\n",
    "# less than 1% dropped\n",
    "print(f\"{df.shape[0] - td.shape[0]} duplicate rows dropped. Thats {df.shape[0] / td.shape[0] * 100}%\")\n",
    "print(f\"{td.shape[0]} rows remain.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48306fc4-0e8e-46c5-a67f-0d2951163a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "for col in td.columns:\n",
    "    missing = td[col].isna().sum()\n",
    "    print(f\"Missing values in {col}: {missing}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf0d9b-ef52-496f-9989-8a6c5419eff4",
   "metadata": {},
   "source": [
    "# Checking for zeros in numeric columns\n",
    "def check_for_zeros(td):\n",
    "    for col in td.columns:\n",
    "        zeros = td[td[col] == 0].shape[0]\n",
    "        print(f\"Zeros in {col}:{zeros}\")\n",
    "        \n",
    "check_for_zeros(td)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f273b588-02c8-4418-969a-71cc86479e9f",
   "metadata": {},
   "source": [
    "Okay, first issue.\n",
    "passenger_count, trip_distance, far_amount and total_amount all contain zeros. It doesn't appear to be a large amount of the overal data. I'd normally check what the percentage of the entire dataset these values account for, however, passenger_count has already been determined to not be kept and the rest of the information can't really be filled in with any mathematical methods because each of the values depends on the other to be calculated. Without distance, we can't determine fare amount, even with distance, it's impossible to know which miles were driven above the 12mph threshold and which were below. There isn't much of a choice but to drop these, however, total_amount can be corrected by simply adding all of the charge column values together, so I'll keep and fix these rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1092dae-1df1-4ac1-9fb6-e242938822ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping rows with 0 values in columns where 0 is not allowed\n",
    "td = td.drop(['passenger_count'], axis=1)\n",
    "td = td[td['trip_distance'] > 0]\n",
    "td = td[td['fare_amount'] > 0]\n",
    "\n",
    "check_for_zeros(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c641cb-0e7f-4ded-a701-b65921ad7f16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how much of the original data ramains\n",
    "remaining = td.shape[0] / df.shape[0] * 100\n",
    "print(f\"Remaining amount of original dataset: {remaining}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ea6d2c-a374-4660-94a6-f3a8a89c50e8",
   "metadata": {},
   "source": [
    "Data Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b51021-e019-40ec-9904-3f5f1ae4726f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to an actual Python/Pandas datetime object ensures that the data is a valid datetime. Then we can move on to exploring the datetimes available.\n",
    "td['pickup_datetime'] = pd.to_datetime(td['pickup_datetime'])\n",
    "td['dropoff_datetime'] = pd.to_datetime(td['dropoff_datetime'])\n",
    "\n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46013281-e332-4360-b3fa-c0e32419a1f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "td['year'] = pd.to_datetime(td['pickup_datetime']).dt.year\n",
    "td['month'] = pd.to_datetime(td['pickup_datetime']).dt.month\n",
    "td['day'] = pd.to_datetime(td['pickup_datetime']).dt.day\n",
    "td['day_of_week'] = pd.to_datetime(td['pickup_datetime']).dt.dayofweek\n",
    "td['hour_of_day'] = pd.to_datetime(td['pickup_datetime']).dt.hour\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f57344cb-0609-4880-81f3-5633f1830952",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the datetime columns to a numpy array for vectorization\n",
    "pickup_array = td['pickup_datetime'].values\n",
    "dropoff_array = td['dropoff_datetime'].values\n",
    "\n",
    "# Getting the new timedelta, this takes less than a second to complete compared to 15+ minutes with apply()\n",
    "trip_duration = np.subtract(dropoff_array, pickup_array)\n",
    "\n",
    "# Adding the resulting array to the dataframe in the trip_duration column\n",
    "td['trip_duration'] = pd.Series(trip_duration)\n",
    "\n",
    "# Converting the timedelta to number of seconds\n",
    "td['trip_duration'] = td['trip_duration'].dt.total_seconds()\n",
    "\n",
    "# Preview the results\n",
    "td.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf029987-a0da-4810-aced-1205b9d8911c",
   "metadata": {},
   "outputs": [],
   "source": [
    "td.drop(['pickup_datetime', 'dropoff_datetime'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e699582-2ce0-4a23-b718-a736ace5f75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "td.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a0dbff-26ef-4ec7-b52c-3bae46405d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = td[td['trip_duration'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05d389e7-ef81-41e2-854d-21a5210e81b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_years = td.year.unique()\n",
    "print(list_of_years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8142d9e-dd57-4fbb-a09b-826c196a5c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in list_of_years:\n",
    "    year_amount = td[td['year'] == year].shape[0]\n",
    "    total_amount = td.shape[0]\n",
    "    \n",
    "    print(f\"{year} makes up {(year_amount / total_amount) * 100}% of the dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "721d6284-c53c-41ed-a3dc-677626049e86",
   "metadata": {},
   "source": [
    "### Unexpected Results\n",
    "\n",
    "It's clear that this dataset is VERY HEAVILY weighted towards 2018. For that reason, dropping anything from before 2018 can help avoid skewing the data towards old trends, while keeping anything newer than 2018 might reveal new trends, although, I don't think it will be super useful. If a dataset of this size consists of 99% of the same year, it's likely that the trips from newer years are either invalid data upon collection, and incomplete enough to actually show any trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "982752c0-b9df-4f10-8ce6-568887edf163",
   "metadata": {},
   "outputs": [],
   "source": [
    "td = td[td['year'] == 2018]\n",
    "\n",
    "td.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f15f82-b884-412b-a6f3-1323b812eeca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating total amounts and dropping rows whose values don't \"add up\"...\n",
    "fare = td['fare_amount'].values\n",
    "extra = np.add(fare, td['extra'].values)\n",
    "mta_tax = np.add(extra, td['mta_tax'].values)\n",
    "tip_amount = np.add(mta_tax, td['tip_amount'].values)\n",
    "imp_surcharge = np.add(tip_amount, td['imp_surcharge'].values)\n",
    "calculated_total_amount = np.add(imp_surcharge, td['tolls_amount'].values)\n",
    "\n",
    "td['calculated_total_amount'] = pd.Series(calculated_total_amount)\n",
    "\n",
    "# validate calculated total by manually adding all relevant columns and comparing to the calculated column\n",
    "td.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8763ffca-efd6-47d0-97bf-70af1b897985",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping anything that isn't correct\n",
    "td = td[td['total_amount'] != td['calculated_total_amount']]\n",
    "\n",
    "td.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c15314d-ae6d-4549-b1d4-37535a4eccb2",
   "metadata": {},
   "source": [
    "### Finishing Up\n",
    "That one total_amount column did a lot more than just clean totals, but it actually checked all of the other total effecting columns at the same time. If any errors occurred in any column, the calculated total would have differed from the calculated total. Missing mta_tax? Dropped. Incorrect amount of tolls? Dropped.\n",
    "\n",
    "That about sums up the cleaning process. While there are still some values that need to be looked further into, such as storoe_and_fwd_flag, anything from this point forward will rely heavily on the exploration phase to understand and determine what to do with. For now, I'll save the cleaned data as a csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "510ebb01-4c88-4777-823b-67b8dc50aa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "td.to_csv('cleaned_nyc_taxi_data_2018.csv')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f67a1b-086d-48d7-b19a-3b01a16d129e",
   "metadata": {},
   "source": [
    "## Excersise\n",
    "\n",
    " - Basic Exploration → Missing values, summary stats.\n",
    "\n",
    " - Data Cleaning → Convert datatypes, handle unrealistic values.\n",
    "\n",
    " - NumPy Operations → Moving averages, per-mile calculations.\n",
    "\n",
    " - Pandas Grouping → Aggregating trip counts and fares.\n",
    "\n",
    " - Advanced Analytics → Outlier detection, busiest locations.\n",
    "\n",
    " - Data Visualization → Line plots, heatmaps.\n",
    "\n",
    " - Bonus (ML) → Train a basic fare prediction model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e45c221-c167-4a8f-8a6c-ef8587c287b8",
   "metadata": {},
   "source": [
    "Identify the top 5 busiest pickup locations and visualize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375edbdd-e90a-48c0-aacd-d33c34907e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_pickup_locations = td['pickup_location_id'].value_counts().head(5)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(8, 5))\n",
    "sns.barplot(x=top_pickup_locations.index, y=top_pickup_locations.values, palette=\"viridis\")\n",
    "plt.xlabel(\"Pickup Location ID\")\n",
    "plt.ylabel(\"Number of Pickups\")\n",
    "plt.title(\"Top 5 Busiest Pickup Locations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68087e55-153f-4cdf-8aee-0159b6e03654",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers in trip duration using box plots and standard deviation\n",
    "#Data Cleaning & Preprocessing\n",
    "\n",
    "#Filter out unrealistic trip durations (e.g., < 1 min or > 24 hours):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "adc2c326-bbe4-48e4-a3da-dcdbdd123033",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'td' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m td = \u001b[43mtd\u001b[49m[(td[\u001b[33m'\u001b[39m\u001b[33mtrip_duration\u001b[39m\u001b[33m'\u001b[39m] > \u001b[32m1\u001b[39m) & (td[\u001b[33m'\u001b[39m\u001b[33mtrip_duration\u001b[39m\u001b[33m'\u001b[39m] < \u001b[32m1440\u001b[39m)]\n",
      "\u001b[31mNameError\u001b[39m: name 'td' is not defined"
     ]
    }
   ],
   "source": [
    "td = td[(td['trip_duration'] > 1) & (td['trip_duration'] < 1440)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09751deb-753b-4afd-800a-3c2b17709d62",
   "metadata": {},
   "source": [
    "NumPy-Based Exercises\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43dd879a-5136-4333-ab96-34f7fbaaafa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute the average fare per mile using NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b497b6a-eb38-41d3-a87c-210c5c3383d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "fares_per_mile = np.where(trips['trip_distance'] > 0, trips['fare_amount'] / trips['trip_distance'], np.nan)\n",
    "print(np.nanmean(fares_per_mile))  # Average fare per mile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de363d79-ddfa-48b6-af38-6ace8925a1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute a moving average of fares over time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a14d8d-88da-4248-a1ed-617548d29b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_avg(arr, window):\n",
    "    return np.convolve(arr, np.ones(window)/window, mode='valid')\n",
    "\n",
    "avg_fares = moving_avg(trips['fare_amount'].dropna().values, 5)\n",
    "print(avg_fares[:10])  # First 10 values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9881d0cf-20d5-4cf5-ac58-740ec71b821b",
   "metadata": {},
   "source": [
    "Data Aggregation & Grouping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be041796-b034-4057-86e0-6f1a65cb9052",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find average fare by hour of the day:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "383955ee-41be-40ad-9297-f7fdb491b25c",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips['hour'] = trips['pickup_datetime'].dt.hour\n",
    "avg_fare_per_hour = trips.groupby('hour')['fare_amount'].mean()\n",
    "print(avg_fare_per_hour)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3635e62f-a139-4577-a3ed-059c66615f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find total trips per day of the week:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1e304e-1a95-40c7-af62-a42cab1ed575",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips['day_of_week'] = trips['pickup_datetime'].dt.dayofweek\n",
    "trips_per_day = trips.groupby('day_of_week')['pickup_datetime'].count()\n",
    "print(trips_per_day)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e850405-3652-44b6-978f-6997680d4018",
   "metadata": {},
   "source": [
    "Advanced Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9507d52-337b-4b5e-b340-4a64bef3e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find the busiest pickup locations over time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69487f3-6873-4d81-86a5-955ed1eed957",
   "metadata": {},
   "outputs": [],
   "source": [
    "busiest_hours = trips.groupby(['pickup_location_id', 'hour']).size().reset_index(name='count')\n",
    "busiest_hours = busiest_hours.sort_values(by='count', ascending=False).head(10)\n",
    "print(busiest_hours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "610a3a8d-ec64-4cff-b0e0-1b42dae8f259",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detect outliers in fare amounts using Z-score:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c075eb3c-0cf1-4e9e-b014-78a33fc0277c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "trips['fare_zscore'] = stats.zscore(trips['fare_amount'])\n",
    "outliers = trips[trips['fare_zscore'].abs() > 3]  # 3 standard deviations away\n",
    "print(outliers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5a2a005-2c01-4ee5-8e20-73bff48db527",
   "metadata": {},
   "source": [
    "Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "22c7eae0-de5c-4787-b10f-eb02c1ba788d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot fare trends by time of day using Seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b3ce6-8045-4dce-a2e0-c528cc5b8d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.lineplot(x=avg_fare_per_hour.index, y=avg_fare_per_hour.values, marker='o')\n",
    "plt.xlabel(\"Hour of Day\")\n",
    "plt.ylabel(\"Average Fare\")\n",
    "plt.title(\"Average Taxi Fare by Hour of the Day\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "121a4d69-7579-4ca8-be99-c58ed967169f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot a heatmap of correlations between numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79a86af-e64a-4295-b598-15a26059453a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = trips[['fare_amount', 'trip_distance', 'trip_duration']].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap=\"coolwarm\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f191549f-2c34-4698-8ac1-5166056c5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d421cd9-5162-469f-9d56-2ec7f26b697d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a simple fare prediction model using Linear Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6283a491-50e2-41f0-842e-d3d93c285f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "features = trips[['trip_distance', 'passenger_count']]\n",
    "target = trips['fare_amount']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "print(predictions[:10])  # First 10 predictions\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
